{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity features (Version 1)\n",
    "This program implements a Linear Regression model with 4 features. The 4 features are the Cosine Similarity values of the query with each of product description, product title, product attribute name, product attribute value.\n",
    "\n",
    "### Overview of the methodology:\n",
    "\n",
    "It first builds the vocab of words under a column and then finds the representations of the words in each training example. After that, it computes the tfidf vectors of the columns in question. The key part of the implementation is that it uses both unigrams and bigrams of words to find the TfIdf vector representation. With these tfidf values in hand, the tfidf values of the search query are computed. There are ultimately 2 vectors - document and search vectors. Each feature is the cosine similarity of a document vector with the corresponding search vector.\n",
    "\n",
    "### Bigram feature engineering:\n",
    "It makes more sense to check for a word in a context. If a pair of words appears consecutively in a document in a certain order and also the search query, it's only fair that there be an added similarity between the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import linear_model\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method takes a couple of vectors - the document and search vectors and returns the cosine similarity between them. The first step is to fill in the values of the search vector. The vectors are both sparse representations of the tfidf values of the words in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the document and search vectors and returns the cosine similarity of the 2 vectors.\n",
    "def computeCosineSim(words, doc_vector, search_vector, vectorizer):\n",
    "    # First store the term frequencies of the words\n",
    "    dict = {}\n",
    "    for word in words:\n",
    "        if word in dict:\n",
    "            count = dict.get(word)\n",
    "            count = count + 1\n",
    "            dict[word] = count\n",
    "        else:\n",
    "            dict[word] = 1\n",
    "\n",
    "    doc_array = doc_vector.toarray()\n",
    "    search_array = search_vector.toarray()\n",
    "\n",
    "    # Get the idf of the vocab.\n",
    "    idf = vectorizer.idf_\n",
    "\n",
    "    # Now compute the tfidf of the words wrt to the document to get the search vector.\n",
    "    # Tfidf of a word in the search query = (tf of the word) * (idf of the word in the document corpus)\n",
    "    for word in dict:\n",
    "        index = vectorizer.vocabulary_.get(word, -1)\n",
    "        if index != -1:\n",
    "            search_array[0][index] = (idf[index] * dict[word]) / len(words)\n",
    "        else:\n",
    "            search_array[0][index] = 0\n",
    "\n",
    "        # Computing the tfidf of the bigrams with this word as the first word.\n",
    "        for word2 in dict:\n",
    "            index1 = vectorizer.vocabulary_.get(word + \" \" + word2, -1)\n",
    "            if index1 != -1:\n",
    "                search_array[0][index1] = (idf[index1] * (dict[word] + dict[word2])) / len(words)\n",
    "            else:\n",
    "                search_array[0][index1] = 0\n",
    "    return cosine_similarity(search_array, doc_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataframe, features matrix (which is a set of cosine similarities for each training/test example) and a file, this method writes the feature values in the given csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def writeFeaturesToFile(df, features, myfile):\n",
    "    myfile.write(\"\\\"product_uid\\\"\" + \",\\\"cosine_desc\\\"\" + \",\\\"cosine_title\\\"\" + \",\\\"cosine_attName\\\"\" + \",\\\"cosine_attValue\\\"\\n\")\n",
    "    for i in range(0, len(features)):\n",
    "        myfile.write(str(df['product_uid'][i]) + \",\" + str(features[i][0]) + \",\" + str(features[i][1]) + \",\" + str(features[i][2]) + \",\" + str(features[i][3]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in extracting the features is merging the files. There are 3 files to work on in the preprocessing step - attributes.csv, train.csv, product_descriptions.csv. Motivation - We need a combined vocab of descriptions, titles and attributes in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method takes the different product files and combines them into one on product_id.\n",
    "def preprocess(isTest):\n",
    "    # Combining the rows of the attributes file on product_uid.\n",
    "    if not isTest:\n",
    "        myfile_att = open(\"att_mod.csv\", \"w\")\n",
    "        df_attributes = pd.read_csv(\"attributes.csv\", encoding='latin-1')\n",
    "        df_attributes['name'] = df_attributes['name'].astype(str) + \" \"\n",
    "        df_attributes['value'] = df_attributes['value'].astype(str) + \" \"\n",
    "        df_attributes = df_attributes.groupby('product_uid').apply(lambda x: x.sum())\n",
    "        df_attributes = df_attributes.drop(df_attributes.columns[[0]], axis=1)\n",
    "        df_attributes.to_csv(myfile_att, sep=',', quoting=csv.QUOTE_NONNUMERIC)\n",
    "        myfile_att.close()\n",
    "\n",
    "        df_attributes = pd.read_csv(\"att_mod.csv\", encoding='latin-1')\n",
    "        df_prodDesc = pd.read_csv(\"product_descriptions.csv\", encoding='latin-1')\n",
    "        result = pd.merge(pd.DataFrame(df_prodDesc), pd.DataFrame(df_attributes), on='product_uid', how='left')\n",
    "        merged_file = open(\"merged.csv\", \"w\")\n",
    "        result.to_csv(merged_file, sep=',', quoting=csv.QUOTE_NONNUMERIC)\n",
    "        merged_file.close()\n",
    "\n",
    "        # Merge the prod_titles in the train file with the product details.\n",
    "        df_train = pd.read_csv(\"train.csv\", encoding='latin-1')\n",
    "        df_merged = pd.read_csv(\"merged.csv\", encoding='latin-1')\n",
    "        result = pd.merge(pd.DataFrame(df_merged),\n",
    "            pd.DataFrame(df_train)[['product_uid', 'product_title','search_term','relevance']], on='product_uid', how='inner')\n",
    "        merged_file = open(\"training.csv\", \"w\")\n",
    "        result.to_csv(merged_file, sep=',', quoting=csv.QUOTE_NONNUMERIC)\n",
    "        merged_file.close()\n",
    "    else:\n",
    "        df_attributes = pd.read_csv(\"att_mod.csv\", encoding='latin-1')\n",
    "        df_prodDesc = pd.read_csv(\"product_descriptions.csv\", encoding='latin-1')\n",
    "        result = pd.merge(pd.DataFrame(df_prodDesc), pd.DataFrame(df_attributes), on='product_uid', how='left')\n",
    "        merged_file = open(\"merged_test.csv\", \"w\")\n",
    "        result.to_csv(merged_file, sep=',', quoting=csv.QUOTE_NONNUMERIC)\n",
    "        merged_file.close()\n",
    "\n",
    "        # Merge the prod_titles in the train file with the product details.\n",
    "        df_test = pd.read_csv(\"test.csv\", encoding='latin-1')\n",
    "        df_merged = pd.read_csv(\"merged_test.csv\", encoding='latin-1')\n",
    "        result = pd.merge(pd.DataFrame(df_merged),\n",
    "            pd.DataFrame(df_test)[['product_uid', 'product_title','search_term']], on='product_uid', how='inner')\n",
    "        merged_file = open(\"testing.csv\", \"w\")\n",
    "        result.to_csv(merged_file, sep=',', quoting=csv.QUOTE_NONNUMERIC)\n",
    "        merged_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the corpus:\n",
    "This method takes care of vectorizing the text under a certain column in a csv file. The tfidfVectorizer() builds and fits the vocab. Each training example can then be transformed before finding the cosine similarity value.\n",
    "\n",
    "The vector of each column (ex: Product description) consists of all the unigrams (individual words) and bigrams (pairs of adjacent words) that appear in all rows of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the column to vectorize, this function build the vocab from words under\n",
    "# the column, gets the tfidf vector representation, computes and returns the cosine similarity matrix.\n",
    "def vectorize(column):\n",
    "    df_merged.fillna(' ', inplace=True)\n",
    "    text = df_merged[column].values.astype('U')\n",
    "\n",
    "    # create the transform - Using both unigrams and bigrams\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(text)\n",
    "\n",
    "    # X = vectorizer.fit_transform(text)\n",
    "    # print(X.toarray())\n",
    "\n",
    "    cosine = []\n",
    "    # Encode each document based on the transform.\n",
    "    for t in range(0, len(df_merged)):\n",
    "        # Encode document\n",
    "        doc_vector = vectorizer.transform([df_merged[column][t]])\n",
    "        # print(doc_vector.toarray())\n",
    "\n",
    "        # Now get the TFIDF of the search query.\n",
    "        search_vector = vectorizer.transform([df_merged['search_term'][t]])\n",
    "        words = re.findall(r\"\\w+\", str(df_merged['search_term'][t]))\n",
    "        cos = computeCosineSim(words, doc_vector, search_vector, vectorizer)\n",
    "\n",
    "        cosine.append(cos)\n",
    "    return cosine\n",
    "\n",
    "\n",
    "# Returns the cosine Similarities of the specified column in the test file with the search query.\n",
    "def vectorizeTest(column):\n",
    "    df_merged.fillna(' ', inplace=True)\n",
    "    df_test_merged.fillna(' ', inplace=True)\n",
    "    text = df_merged[column].values.astype('U')\n",
    "\n",
    "    # create the transform - Using both unigrams and bigrams\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(text)\n",
    "\n",
    "    cosine_test = []\n",
    "    # Encode each document based on the transform.\n",
    "    for t in range(0, len(df_test_merged)):\n",
    "        doc_vector = vectorizer.transform([df_test_merged[column][t]])\n",
    "\n",
    "        # Now get the TFIDF of the search query.\n",
    "        search_vector = vectorizer.transform([df_test_merged['search_term'][t]])\n",
    "        words = re.findall(r\"\\w+\", str(df_test_merged['search_term'][t]))\n",
    "        # words = str(df_test_merged['search_term'][t]).split(\" \")\n",
    "        cos = computeCosineSim(words, doc_vector, search_vector, vectorizer)\n",
    "        cosine_test.append(cos)\n",
    "    return cosine_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow of the program:\n",
    "\n",
    "The following section is the driver of the program.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1) Preprocess the train and test files to get a merged file on the train and test data.\n",
    "\n",
    "2) Compute the cosine similarities of the queries (search terms) with product description, product title, product attribute name, product attribute value.\n",
    "\n",
    "3) Receive 4 values for each training example from the above step.\n",
    "\n",
    "4) Learn a Linear Regression model based on the input and output.\n",
    "    Input = 4 Cosine Similarity values\n",
    "    Output = Relevance values for each training row\n",
    "    \n",
    "5) Get the test data.\n",
    "\n",
    "6) Predict using Linear Regression.\n",
    "\n",
    "7) Print the predictions to submission.csv.\n",
    "\n",
    "\n",
    "## Result:\n",
    "### RMSE = 0.52134\n",
    "#### Coefficients of Linear Regression Model: [ 0.86960024,  0.70365176,  0.31511568, -0.42450679]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the training file\n",
    "preprocess(False)\n",
    "# Preprocess the test file\n",
    "preprocess(True)\n",
    "\n",
    "df_merged = pd.read_csv(\"training.csv\", encoding='latin-1')\n",
    "df_test_merged = pd.read_csv(\"testing.csv\", encoding='latin-1')\n",
    "\n",
    "features = []\n",
    "features_test = []\n",
    "\n",
    "cosine = vectorize('product_description')\n",
    "cosine_test = vectorizeTest('product_description')\n",
    "\n",
    "for c in cosine:\n",
    "    temp = []\n",
    "    temp.append(c[0][0])\n",
    "    features.append(temp)\n",
    "\n",
    "for c in cosine_test:\n",
    "    temp = []\n",
    "    temp.append(c[0][0])\n",
    "    features_test.append(temp)\n",
    "\n",
    "\n",
    "cosine = vectorize('product_title')\n",
    "cosine_test = vectorizeTest('product_title')\n",
    "\n",
    "for i in range(0, len(cosine)):\n",
    "    features[i].append(cosine[i][0][0])\n",
    "\n",
    "for i in range(0, len(cosine_test)):\n",
    "    features_test[i].append(cosine_test[i][0][0])\n",
    "\n",
    "\n",
    "cosine = vectorize('name')\n",
    "for i in range(0, len(cosine)):\n",
    "    features[i].append(cosine[i][0][0])\n",
    "\n",
    "\n",
    "cosine_test = vectorizeTest('name')\n",
    "for i in range(0, len(cosine_test)):\n",
    "    features_test[i].append(cosine_test[i][0][0])\n",
    "\n",
    "\n",
    "cosine = vectorize('value')\n",
    "for i in range(0, len(cosine)):\n",
    "    features[i].append(cosine[i][0][0])\n",
    "\n",
    "\n",
    "cosine_test = vectorizeTest('value')\n",
    "for i in range(0, len(cosine_test)):\n",
    "    features_test[i].append(cosine_test[i][0][0])\n",
    "\n",
    "\n",
    "train_features = open(\"train_features.csv\", \"w\")\n",
    "writeFeaturesToFile(df_merged, features, train_features)\n",
    "train_features.close()\n",
    "\n",
    "test_features = open(\"test_features.csv\", \"w\")\n",
    "writeFeaturesToFile(df_test_merged, features_test, test_features)\n",
    "test_features.close()\n",
    "\n",
    "df_relevance = df_merged['relevance']\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "regr.fit(features, df_relevance)\n",
    "print(\"Coefficients: \" + str(regr.coef_))\n",
    "\n",
    "test_predictions = regr.predict(features_test)\n",
    "\n",
    "df_test_merged = pd.read_csv(\"test.csv\", encoding='latin-1')\n",
    "df_test_merged = df_test_merged['id']\n",
    "\n",
    "myfile = open(\"submission.csv\", \"w\")\n",
    "myfile.write(\"\\\"id\\\"\" + \",\\\"relevance\\\"\\n\")\n",
    "for id in range(0, len(df_test_merged)):\n",
    "    myfile.write(str(df_test_merged[id]) + \",\" + str(test_predictions[id]) + \"\\n\")\n",
    "myfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
